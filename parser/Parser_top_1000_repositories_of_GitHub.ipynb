{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ6KhcVykWxq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# --- НАСТРОЙКИ ---\n",
        "# Ваш персональный токен (уже вставлен)\n",
        "TOKEN = 'input your token here'\n",
        "HEADERS = {\"Authorization\": f\"token {TOKEN}\"}\n",
        "MAX_REPOS = 1000\n",
        "REPOS_PER_PAGE = 100\n",
        "\n",
        "def get_top_repositories():\n",
        "    all_repos = []\n",
        "    print(\"--- Этап 1: Сбор метаданных ТОП-1000 репозиториев ---\")\n",
        "\n",
        "    # Идем по страницам (GitHub Search API отдает максимум 1000 результатов)\n",
        "    for page in range(1, 11):\n",
        "        # Запрос: репозитории со звездами > 10000, сортировка по звездам\n",
        "        url = f\"https://api.github.com/search/repositories?q=stars:>10000&sort=stars&order=desc&per_page={REPOS_PER_PAGE}&page={page}\"\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            items = response.json().get('items', [])\n",
        "            all_repos.extend(items)\n",
        "            print(f\"Загружена страница {page}/10. Всего собрано: {len(all_repos)}\")\n",
        "        else:\n",
        "            print(f\"Ошибка на странице {page}: {response.status_code}\")\n",
        "            print(response.json())\n",
        "            break\n",
        "\n",
        "        # Search API очень чувствителен к лимитам, ждем между страницами\n",
        "        time.sleep(5)\n",
        "\n",
        "    return all_repos\n",
        "\n",
        "def analyze_repo_structure(repo):\n",
        "    full_name = repo['full_name']\n",
        "    url_contents = f\"https://api.github.com/repos/{full_name}/contents/\"\n",
        "\n",
        "    res = requests.get(url_contents, headers=HEADERS)\n",
        "\n",
        "    # Инициализируем теги (правила)\n",
        "    has_coc = False\n",
        "    has_contributing = False\n",
        "    has_workflows = False\n",
        "    has_readme = False\n",
        "\n",
        "    if res.status_code == 200:\n",
        "        content_list = res.json()\n",
        "        filenames = [f['name'].upper() for f in content_list]\n",
        "\n",
        "        # Проверяем наличие ключевых файлов\n",
        "        has_coc = any(\"CODE_OF_CONDUCT\" in name for name in filenames)\n",
        "        has_contributing = any(\"CONTRIBUTING\" in name for name in filenames)\n",
        "        has_readme = any(\"README\" in name for name in filenames)\n",
        "        has_workflows = \".GITHUB\" in filenames # Обычно workflows внутри этой папки\n",
        "\n",
        "    return {\n",
        "        'repo_name': repo['name'],\n",
        "        'organization': full_name.split('/')[0],\n",
        "        'language': repo['language'],\n",
        "        'stars': repo['stargazers_count'],\n",
        "        'forks': repo['forks_count'],\n",
        "        'open_issues': repo['open_issues_count'],\n",
        "        'age_days': (pd.Timestamp.now(tz='UTC') - pd.to_datetime(repo['created_at'])).days,\n",
        "        'has_coc': has_coc,\n",
        "        'has_contributing': has_contributing,\n",
        "        'has_workflows': has_workflows,\n",
        "        'has_readme': has_readme,\n",
        "        'description': repo['description']\n",
        "    }\n",
        "\n",
        "# --- ОСНОВНОЙ ЦИКЛ ---\n",
        "top_repos = get_top_repositories()\n",
        "final_results = []\n",
        "\n",
        "print(\"\\n--- Этап 2: Глубокий анализ структуры каждого репозитория ---\")\n",
        "print(\"Это займет время (около 30-40 минут) из-за лимитов GitHub API.\")\n",
        "\n",
        "for i, repo in enumerate(top_repos):\n",
        "    try:\n",
        "        data = analyze_repo_structure(repo)\n",
        "        final_results.append(data)\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"Обработано {i+1} из {len(top_repos)}...\")\n",
        "\n",
        "        # Пауза, чтобы не превысить лимит 5000 запросов в час для токена\n",
        "        # и лимиты на просмотр содержимого\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке {repo['full_name']}: {e}\")\n",
        "\n",
        "# Сохранение в CSV\n",
        "df = pd.DataFrame(final_results)\n",
        "df.to_csv('top_1000_os_rules.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"\\n--- АНАЛИЗ ЗАВЕРШЕН ---\")\n",
        "print(f\"Файл 'top_1000_os_rules.csv' готов. Всего записей: {len(df)}\")"
      ]
    }
  ]
}